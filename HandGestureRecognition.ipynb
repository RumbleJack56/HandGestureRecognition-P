{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gesture-Recognition System For Drones**\n",
    "\n",
    "## **Goal:**\n",
    "Efficiently using a machine learning model and Mediapipe for hand-landmarking to create a system that can reliably predict hand gestures that pass on movement data to the drone.\n",
    "\n",
    "## **The gestures:**\n",
    "We will be using a total of 8 gestures to make the tello run\n",
    "Gestures used are:\n",
    "<div style=\"display: inline-block; margin-right: 10%; max-width: 200px; float: right;\">\n",
    "<img\n",
    "  src=\"https://s-cdn.ryzerobotics.com/stormsend/uploads/13433930-d1e1-0135-d3c1-12530322f90d/guava-%E7%99%BD-pc-160_154_2x.png\"\n",
    "  alt=\"Drone Image\"\n",
    "  title=\"Fig.1 Drone\"\n",
    "  style=\"border-radius:5%;\"><p align=\"center\">Fig.1 Drone</p>\n",
    "</div>\n",
    "\n",
    "1. ```Up     - Point Upwards                         (2)```\n",
    "2. ```Down   - Point Downwards                       (2)```\n",
    "3. ```Left   - Point to Left                         (2)```\n",
    "4. ```Right  - Point to Right                        (2)```\n",
    "5. ```Front  - Flatten hand and Point Forward        (2)```\n",
    "6. ```Back   - Thumb and Pinky Finger out            (2)```\n",
    "7. ```Land   - Okay sign                             (2)```\n",
    "8. ```Flip   - Yo! sign                              (4)```\n",
    "\n",
    "\n",
    "#### This is  a **four-part project** and the dataset of this is available on kaggle.\n",
    "The github repository of the project is:<br>\n",
    "[https://github.com/RumbleJack56/HandGestureRecognition-P](https://github.com/RumbleJack56/HandGestureRecognition-P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: Data-Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect data using ```opencv``` library and use ```cv2.VideoCapture()``` for accessing camera to take images of different gestures as training examples.\n",
    "\n",
    "* We check for webcams\n",
    "* Then select the webcam\n",
    "* and then we click picture every button press (s) and save it in dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
